{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语料分割成句子并筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "def updateFile(file,separator,remove_elements= [\"\\n\",\"\\\\\",\"（）\",\"＼\",\"★\",\"●\",\"▼\",\"■\",\"➥\",\"」\",\"「\",\" \"],data_index = \"text\"): #remove_elements需移除元素 data_index取出的索引\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f1,open(\"%s.bak\" % file, \"w\", encoding=\"utf-8\") as f2:  #f1为源文件  f2处理后的文件\n",
    "        for line in f1:\n",
    "            data = json.loads(line)[data_index]  #逐行读取json并取出语料\n",
    "            #data = data[data.index(\"\\n\"):]\n",
    "            for remove_element in remove_elements: \n",
    "                data = data.replace(remove_element,'') #删除所有的需删除元素\n",
    "            data_list = data.split(separator) #分隔符，用的是“。”\n",
    "            for text in data_list: # 遍历分割后所有的文本\n",
    "                if len(text)>4:  #忽略文本长度小于等于4的元素\n",
    "                    text_dict = dict(text=text,target = \"通畅\") #声明目标json所需的字典\n",
    "                    data_final =json.dumps(text_dict,ensure_ascii=False) #字典转为str\n",
    "                    f2.write(data_final + \"\\n\")  #写入处理后的文件\n",
    "    os.remove(file) #移除原文件\n",
    "    os.rename(\"%s.bak\" % file, file) #将处理后的文件改名为原文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "def updateFile(file,separator,remove_elements= [\"\\n\",\"\\\\\",\"（）\",\"＼\",\"★\",\"●\",\"▼\",\"■\",\"➥\",\"」\",\"「\",\" \",\"</p>\",\"<p>\",\"<strong>\",\"</strong>\",\"</div>\",\"</div>\"],data_index = \"text\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f1,open(\"%s.bak\" % file, \"w\", encoding=\"utf-8\") as f2:\n",
    "        data_json = f1.read()\n",
    "        data = json.loads(data_json)[data_index]\n",
    "        for remove_element in remove_elements:\n",
    "            data = data.replace(remove_element,'')\n",
    "        data_list = data.split(separator)\n",
    "        for text in data_list:\n",
    "            if len(text)>5:\n",
    "                text_dict = dict(text=text,target = \"通畅\")\n",
    "                data_final =json.dumps(text_dict,ensure_ascii=False)\n",
    "                if bool(re.search(r'\\d',data_final)):\n",
    "                    continue\n",
    "                f2.write(data_final + \"\\n\")\n",
    "    os.remove(file)\n",
    "    os.rename(\"%s.bak\" % file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_process(filepath): #遍历文件夹\n",
    "    for filepath,dirnames,filenames in os.walk(filepath): #遍历文件夹获取文件路径\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(filepath,filename)  #path为文件路径\n",
    "            updateFile(path,\"。\") #调用函数进行处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割成各个部分进行构造不通顺句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_corpus(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        i = random.randint(1,3)\n",
    "        f0 = open(r\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\试验语料\\四处错误.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f1 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词误用.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f2 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词拼写.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f3 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词缺失.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f4 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词啰嗦.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f5 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词语义重复.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f6 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词的词序.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f7 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字形近.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f8 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字音近.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f9 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字缺失.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f10 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字啰嗦.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f11 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字的词序.txt\", \"a\", encoding=\"utf-8\")\n",
    "        for line in f:\n",
    "            # if (i%3)<2:\n",
    "            f0.write(line)\n",
    "            x=random.randint(1,10)\n",
    "            if x!=1:\n",
    "                continue\n",
    "            else:\n",
    "                r=random.randint(1,1000)\n",
    "                if r<=35:\n",
    "                    f1.write(line)\n",
    "                elif r<=350:\n",
    "                    f2.write(line)\n",
    "                elif r<=425:\n",
    "                    f3.write(line)\n",
    "                elif r<=450:\n",
    "                    f4.write(line)\n",
    "                elif r<=475:\n",
    "                    f5.write(line)\n",
    "                elif r<=500:\n",
    "                    f6.write(line)\n",
    "                elif r<=535:\n",
    "                    f7.write(line)\n",
    "                elif r<=850:\n",
    "                    f8.write(line)\n",
    "                elif r<=925:\n",
    "                    f9.write(line)\n",
    "                elif r<=975:\n",
    "                    f10.write(line)\n",
    "                elif r<=1000:\n",
    "                    f11.write(line)\n",
    "            i+=1\n",
    "        for f in f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11:\n",
    "            f0.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将处理后的所有通顺与不通顺句子综合成csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def corpus_integrate_tocsv(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f1,open(r\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\试验语料\\总语料.csv\", \"a\", encoding=\"utf-8\") as f2:\n",
    "        for line in f1:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            text = data[\"text\"]\n",
    "            category = data[\"target\"]\n",
    "            if len(category)==2:\n",
    "                category=\"通顺\"\n",
    "            else:\n",
    "                category=\"不通顺\"\n",
    "            csv_writer = csv.writer(f2)\n",
    "            csv_writer.writerow([text, category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_split(filepath): #遍历文件夹\n",
    "    for filepath,dirnames,filenames in os.walk(filepath): #遍历文件夹获取文件路径\n",
    "            for filename in filenames:\n",
    "                path = os.path.join(filepath,filename)  #path为文件路径\n",
    "                split_corpus(path) #调用函数进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_integrate(folder_path):\n",
    "    with open(r\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\试验语料\\总语料.csv\", \"a\", encoding=\"utf-8\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow([f'text', 'category'])\n",
    "    for filepath,dirnames,filenames in os.walk(folder_path): #遍历文件夹获取文件路径\n",
    "        for filename in filenames:\n",
    "            path = os.path.join(filepath,filename)  #path为文件路径\n",
    "            corpus_integrate_tocsv(path) #调用函数进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath,dirnames,filenames in os.walk(r\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\试验语料\\四处错误\"): #遍历文件夹获取文件路径\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(filepath,filename)  #path为文件路径\n",
    "        split_corpus(path) #调用函数进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_integrate(r\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\试验语料\\实验总语料\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_process(r\"D:\\python_practice\\数据表示\\中文语料\\wiki_zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_process(r\"D:\\python_practice\\数据表示\\中文语料\\news_broadcast_corpus-master\\news_broadcast_corpus-master\\news_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateFile(r\"D:\\python_practice\\数据表示\\中文语料\\weixin_public_corpus-master\\articles\\articles.json\",\"。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_corpus(r\"D:\\python_practice\\数据表示\\中文语料\\语料\\wiki_zh\\AA\\wiki_00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_split(r\"D:\\python_practice\\数据表示\\中文语料\\语料\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eb1db521203f7b822e5c892b02c13cf0191357588be94c2e8d85c6eff67db67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
