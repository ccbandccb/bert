{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 判断是否满足中文句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(string):\n",
    "    \"\"\"\n",
    "    检查整个字符串是否包含中文\n",
    "    :param string: 需要检查的字符串\n",
    "    :return: bool\n",
    "    \"\"\"\n",
    "    s=len(string)\n",
    "    i=0\n",
    "    for ch in string:\n",
    "        if u'\\u4e00' <= ch <= u'\\u9fff':\n",
    "            i+=1\n",
    "    b=i/s\n",
    "    if b>=0.7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 语料分割成句子并筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "def updateFile(file,separator = \"。\",remove_elements= [\"\\n\",\"\\\\\",\" \",\">\"]):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f1,open(\"D:\\python_practice\\数据表示\\新闻语料\\新闻总语料.txt\", \"a\", encoding=\"utf-8\") as f2:\n",
    "        for line in f1:\n",
    "            if \"。\"  not in line:\n",
    "                continue\n",
    "            if not is_chinese(line):\n",
    "                continue\n",
    "            data = line.strip()\n",
    "            for remove_element in remove_elements:\n",
    "                data = data.replace(remove_element,'')\n",
    "            data_list = data.split(separator)\n",
    "            for text in data_list:\n",
    "                if len(text)>5:\n",
    "                    text_dict = dict(text=text,target = \"通顺\")\n",
    "                    data_final =json.dumps(text_dict,ensure_ascii=False)\n",
    "                    f2.write(data_final + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath,dirnames,filenames in os.walk(r\"C:\\Users\\MNH\\Desktop\\THUCNews\"): #遍历文件夹获取文件路径\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(filepath,filename)  #path为文件路径\n",
    "        updateFile(path) #调用函数进行处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分割成各个部分进行构造不通顺句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def split_corpus(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        f0 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\通顺总数据.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f1 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\同音错别字.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f2 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\形近错别字.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f3 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\字重复不通顺.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f4 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\缺字不通顺.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f5 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\词重复不通顺.txt\", \"a\", encoding=\"utf-8\")\n",
    "        f6 = open(\"D:\\python_practice\\数据表示\\中文语料\\分割语料\\多字不通顺.txt\", \"a\", encoding=\"utf-8\")\n",
    "        need = []\n",
    "        for line in f:\n",
    "            need.append(line)\n",
    "        all_data = random.sample(need,1000000)\n",
    "        for line in all_data:\n",
    "            f0.write(line)\n",
    "            r=random.randint(1,10)\n",
    "            if r<=4:\n",
    "                f1.write(line)\n",
    "            elif r<=6:\n",
    "                f2.write(line)\n",
    "            elif r<=7:\n",
    "                f3.write(line)\n",
    "            elif r<=8:\n",
    "                f4.write(line)\n",
    "            elif r<=9:\n",
    "                f5.write(line)\n",
    "            elif r<=10:\n",
    "                f6.write(line)\n",
    "        for f in f1,f2,f3,f4,f5,f6:\n",
    "            f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将处理后的所有通顺与不通顺句子综合成csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def corpus_integrate_tocsv(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f1,open(r\"J:\\bert数据\\总数据.csv\", \"a\", encoding=\"utf-8\") as f2:\n",
    "        csv_writer = csv.writer(f2)\n",
    "        csv_writer.writerow([\"text\", \"category\"])\n",
    "        for line in f1:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "            except:\n",
    "                continue\n",
    "            try:\n",
    "                smooth = data[\"通顺\"]\n",
    "                smooth_not = data[\"不通顺\"]\n",
    "            except:\n",
    "                smooth = data[\"通畅\"]\n",
    "                smooth_not = data[\"不通畅\"]\n",
    "            csv_writer.writerow([smooth, \"通顺\"])\n",
    "            csv_writer.writerow([smooth_not, \"不通顺\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath,dirnames,filenames in os.walk(r\"J:\\bert数据\\处理完\"): #遍历文件夹获取文件路径\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(filepath,filename)  #path为文件路径\n",
    "        corpus_integrate_tocsv(path) #调用函数进行处理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eb1db521203f7b822e5c892b02c13cf0191357588be94c2e8d85c6eff67db67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
