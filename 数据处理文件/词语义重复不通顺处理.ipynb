{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件作为句子不通顺处理的代码文件之一，包含词语义重复和近义词替换两个函数，函数在处理过程中可能有5%的损耗，因此在处理句子时，多引入一个文件存储未处理句子，并对这些句子进行第二次处理，降低总体损耗率。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonyms\n",
    "\n",
    "Chinese Synonyms for Natural Language Processing and Understanding.\n",
    "\n",
    "更好的中文近义词：聊天机器人、智能问答工具包。\n",
    "\n",
    "synonyms可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等。\n",
    "\n",
    "https://gitcode.net/mirrors/huyingxi/synonyms?utm_source=csdn_github_accelerator#synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_open library not found; falling back to local-filesystem-only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\vocab.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\CCB\\AppData\\Local\\Temp\\jieba.uc6efccf6b79794ce2a412e8c315990a3.cache\n",
      "Loading model cost 1.401 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\words.vector.gz] ...\n",
      "Vocab size in vector model: 435729\n",
      "model_path: c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\words.vector.gz\n",
      "version: 3.18.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 435729,\n",
       " 'version': '3.18.0',\n",
       " 'model_path': 'c:\\\\Users\\\\CCB\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\synonyms\\\\data\\\\words.vector.gz'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import synonyms\n",
    "# 打印当前包的描述信息\n",
    "synonyms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['这', '是', '一个', '通顺', '句子'], ['r', 'v', 'm', 'nr', 'n'])\n",
      "(['通顺', '浅显', '文句', '浅易', '简洁明了', '案续', 'trained', '民泽', '圣塞雷县', 'Gazeille'], [1.0, 0.5761281, 0.5426158, 0.47484627, 0.4415007, 0.41417015, 0.35835257, 0.23456658, 0.18564261, 0.16851546])\n",
      "(['测试', '试验', '检测', '验证', '测验', '实验', '试车', '测试报告', '测试表明', '测试方法'], [1.0, 0.8104965, 0.6860482, 0.6619119, 0.6416688, 0.6328682, 0.6220212, 0.61274487, 0.59768546, 0.56227165])\n"
     ]
    }
   ],
   "source": [
    "print(synonyms.seg('这是一个通顺句子'))  # 该分词不去停用词和标点\n",
    "print(synonyms.nearby('通顺'))\n",
    "print(synonyms.nearby('测试'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_open library not found; falling back to local-filesystem-only\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\vocab.txt ...\n",
      "Loading model from cache C:\\Users\\CCB\\AppData\\Local\\Temp\\jieba.uc6efccf6b79794ce2a412e8c315990a3.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 3.475 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [c:\\Users\\CCB\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\synonyms\\data\\words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import synonyms\n",
    "import random\n",
    "\n",
    "# 定义一个函数获取一个句子中某一个词的近义词表\n",
    "def synonyms_list_get(sentence):\n",
    "  if sentence:\n",
    "    # 句子分词\n",
    "    word_list = synonyms.seg(sentence)[0]\n",
    "    \n",
    "    # 随机选择一个中文词语，获取其近义词列表\n",
    "    word = 'w'\n",
    "    synonyms_list = []\n",
    "    n=0\n",
    "    while is_all_chinese(word) is False and n<2:\n",
    "      word = random.choice(word_list)\n",
    "      synonyms_list = synonyms.nearby(word)[0]\n",
    "      n += 1\n",
    "\n",
    "    if is_all_chinese(word) is False or synonyms_list == []:\n",
    "      word_list = ['陈']\n",
    "      word = '陈'\n",
    "      synonyms_list = ['陈']\n",
    "    \n",
    "    #返回分词列表、被选中的词、被选中词的近义词列表\n",
    "    return word_list, word, synonyms_list\n",
    "\n",
    "  else:\n",
    "      word_list = ['陈']\n",
    "      word = '陈'\n",
    "      synonyms_list = ['陈']\n",
    "    \n",
    "      #返回分词列表、被选中的词、被选中词的近义词列表\n",
    "      return word_list, word, synonyms_list\n",
    "\n",
    "#检验是否全是中文字符\n",
    "def is_all_chinese(strs):\n",
    "  for _char in strs:\n",
    "    if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "      return False\n",
    "  return True\n",
    "\n",
    "# 定义一个函数，用于替换句子中的词语\n",
    "def replace_word(sentence):\n",
    "  word_list, word, synonyms_list = synonyms_list_get(sentence)\n",
    "  index = word_list.index(word)\n",
    "  \n",
    "  # 随机寻找近义词，且仅使用中文近义词进行替换\n",
    "  word_replaced = 'w'\n",
    "  while is_all_chinese(word_replaced) is False :\n",
    "    word_replaced = random.choice(synonyms_list)\n",
    "    \n",
    "  word_list[index] = word_replaced\n",
    "\n",
    "  # 将替换后的词语列表拼接为句子，并返回\n",
    "  return \"\".join(word_list)\n",
    "\n",
    "\n",
    "def add_word(sentence):\n",
    "  word_list, word, synonyms_list = synonyms_list_get(sentence)\n",
    "  index = word_list.index(word)\n",
    "\n",
    "  # 随机寻找近义词，且仅使用中文近义词进行替换词\n",
    "  word_added = 'w'\n",
    "  while is_all_chinese(word_added) is False:\n",
    "    word_added = random.choice(synonyms_list)\n",
    "\n",
    "  # 在句子中的随机位置插入近义词\n",
    "  add_index = random.randint(0,len(word_list))\n",
    "  word_list.insert(add_index,word_added)\n",
    "\n",
    "  # 将添加后的词语列表拼接为句子，并返回\n",
    "  return \"\".join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "陈\n",
      "陈陈\n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "\n",
    "print(replace_word(sentence))\n",
    "print(add_word(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f1 = open('词语义重复不通顺.txt','r',encoding='UTF-8')\n",
    "f2 = open(\"词语义重复不通顺（处理完)2.txt\",\"w\",encoding='UTF-8')\n",
    "f3 = open(\"词语义重复丢弃.txt\",\"w\",encoding='UTF-8')\n",
    "\n",
    "for line in f1:\n",
    "    try:\n",
    "        line1 = line.strip()\n",
    "        line2 = line1.replace('\\\\','\\\\\\\\')\n",
    "        line3 = json.loads(line2)\n",
    "        line4 = line3[\"text\"]\n",
    "        line5 = add_word(line4)\n",
    "        if line5 == '陈' or line5 == '陈陈':\n",
    "            f3.write(json.dumps(line3, ensure_ascii=False)+'\\n')\n",
    "        else:\n",
    "            dic = {}\n",
    "            dic[\"通顺\"] = line4\n",
    "            dic[\"不通顺\"] = line5\n",
    "            f2.write(json.dumps(dic, ensure_ascii=False)+'\\n')\n",
    "    except:\n",
    "        continue\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f1 = open('词重复丢弃.txt','r',encoding='UTF-8')\n",
    "f2 = open(\"词重复丢弃处理.txt\",\"w\",encoding='UTF-8')\n",
    "\n",
    "for line in f1:\n",
    "    try:\n",
    "        line = line.strip()\n",
    "        line = line.replace('\\\\','\\\\\\\\')\n",
    "        line = json.loads(line)\n",
    "        line = line[\"text\"]\n",
    "        line1 = add_word(line)\n",
    "        if line1 == '陈' or line1 == '陈陈':\n",
    "            continue\n",
    "        else:\n",
    "            dic = {}\n",
    "            dic[\"通顺\"] = line\n",
    "            dic[\"不通顺\"] = line1\n",
    "            f2.write(json.dumps(dic, ensure_ascii=False)+'\\n')\n",
    "    except:\n",
    "        continue\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f1 = open('测试.txt','r',encoding='UTF-8')\n",
    "f2 = open(\"测试（处理完）.txt\",\"w\",encoding='UTF-8')\n",
    "for line in f1:\n",
    "    try:\n",
    "        line = line.strip()\n",
    "        line = line.replace('\\\\','\\\\\\\\')\n",
    "        line = json.loads(line)\n",
    "        line = line[\"text\"]\n",
    "        line1 = replace_word(line)\n",
    "        if line1 == '陈' or line1 == '陈陈':\n",
    "            continue\n",
    "        else:\n",
    "            dic = {}\n",
    "            dic[\"通顺\"] = line\n",
    "            dic[\"不通顺\"] = line1\n",
    "            f2.write(json.dumps(dic, ensure_ascii=False)+'\\n')\n",
    "    except:\n",
    "        continue\n",
    "f2.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e364dc6ede94e2e621cd521cdaa792a539ee2b4cddbefab935d220735f3efea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
